# backend/requirements-nlp.txt
# 安装你实际需要使用的本地 NLP 库

# --- 文本切分与处理 ---
nltk>=3.8.0,<3.9.0 # 新增：用于句子切分等NLP任务
# 安装后需要下载模型: python -m nltk.downloader punkt

# --- 中文分词 ---
jieba>=0.42.0,<0.43.0 # 轻量级中文分词库

# --- 其他可选的重型NLP库 (默认不启用) ---
# spacy>=3.7.0,<3.8.0
# # 下载模型: python -m spacy download zh_core_web_sm
# stanza>=1.8.0,<1.9.0
# hanlp>=2.1.0a0,<2.2.0
# snownlp>=0.12.0,<0.13.0